import re
import json
import streamlit as st
from google import genai
from google.genai import types
from prompts import PROMPTS 

@st.cache_data(show_spinner=False)
def get_embedding(text):
    """
    [V246] ì‹ í˜• ë¼ì´ë¸ŒëŸ¬ë¦¬(google-genai) í˜¸í™˜ì„± ê°•í™”
    - ì—¬ëŸ¬ ëª¨ë¸ëª… í˜•ì‹ì„ ìˆœì°¨ì ìœ¼ë¡œ ì‹œë„í•˜ì—¬ ì„±ê³µë¥ ì„ ë†’ì…ë‹ˆë‹¤.
    - ì‹¤íŒ¨ ì‹œ í™”ë©´ì— ì •í™•í•œ ì—ëŸ¬ ì›ì¸ì„ ì¶œë ¥í•©ë‹ˆë‹¤.
    """
    cleaned_text = clean_text_for_db(text)
    
    # 1. í…ìŠ¤íŠ¸ê°€ ì—†ìœ¼ë©´ API í˜¸ì¶œ ë°©ì§€ (ë¹„ìš©/ì—ëŸ¬ ì ˆì•½)
    if not cleaned_text:
        return []

    try:
        # 1. API í‚¤ ë¡œë“œ
        api_key = st.secrets["GEMINI_API_KEY"]
        client = genai.Client(api_key=api_key)
        
        # 2. ì‹œë„í•  ëª¨ë¸ëª… í›„ë³´êµ°
        candidate_models = ["text-embedding-004", "models/text-embedding-004"]
        last_error = None
        
        # 3. ìˆœì°¨ì ìœ¼ë¡œ ì‹œë„
        for model_name in candidate_models:
            try:
                response = client.models.embed_content(
                    model=model_name,
                    contents=cleaned_text
                )
                
                if response.embeddings:
                    return response.embeddings[0].values
                    
            except Exception as e:
                print(f"âš ï¸ ëª¨ë¸ ì‹œë„ ì‹¤íŒ¨ ({model_name}): {e}")
                last_error = e
                continue
        
        # 4. ëª¨ë“  ì‹œë„ê°€ ì‹¤íŒ¨í–ˆì„ ê²½ìš°
        error_msg = f"ğŸš¨ AI ì„ë² ë”© ìƒì„± ì‹¤íŒ¨.\nì›ì¸: {str(last_error)}"
        print(error_msg)
        st.error(error_msg)
        return []

    except Exception as e_fatal:
        st.error(f"ì‹œìŠ¤í…œ ì¹˜ëª…ì  ì˜¤ë¥˜: {str(e_fatal)}")
        return []

def semantic_split_v143(text, target_size=1200, min_size=600):
    flat_text = " ".join(text.split())
    sentences = re.split(r'(?<=[.!?])\s+', flat_text)
    chunks, current_chunk = [], ""
    for sentence in sentences:
        if len(current_chunk) + len(sentence) <= target_size:
            current_chunk += " " + sentence
        else:
            if current_chunk: chunks.append(current_chunk.strip())
            current_chunk = sentence
    if current_chunk:
        if len(current_chunk) < min_size and chunks:
            chunks[-1] = chunks[-1] + " " + current_chunk.strip()
        else: chunks.append(current_chunk.strip())
    return chunks

def clean_text_for_db(text):
    if not text: return ""
    text = text.replace("\u0000", "")
    return "".join(ch for ch in text if ch.isprintable() or ch in ['\n', '\r', '\t']).strip()

def extract_json(text):
    try:
        cleaned = re.sub(r'```json\s*|```', '', text).strip()
        return json.loads(cleaned)
    except: return None

# --------------------------------------------------------------------------------
# [V206] ìë™ í‚¤ì›Œë“œ íƒœê¹…(Auto-Tagging) ì—”ì§„
# --------------------------------------------------------------------------------
def extract_metadata_ai(ai_model, content):
    try:
        prompt = PROMPTS["extract_metadata"].format(content=content[:2000])
        res = ai_model.generate_content(prompt)
        return extract_json(res.text)
    except: return None

@st.cache_data(ttl=3600, show_spinner=False)
def analyze_search_intent
