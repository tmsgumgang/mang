from collections import Counter
# [í•µì‹¬] í†µí•©ëœ ê¸°ëŠ¥(ì¬ê³ +ì¼ì •+ê²Œì‹œíŒ+ê·¸ë˜í”„)ì„ ê°€ì§„ ë¶€ëª¨ í´ë˜ìŠ¤ ì„í¬íŠ¸
from db_collab import DBCollab

# [ìƒì†] DBManagerëŠ” DBCollabì˜ ëª¨ë“  ê¸°ëŠ¥(Inventory, Schedule, Community)ì„ ë¬¼ë ¤ë°›ìŠµë‹ˆë‹¤.
class DBManager(DBCollab):
    def __init__(self, supabase_client):
        self.supabase = supabase_client
        # ë¶€ëª¨ í´ë˜ìŠ¤(DBCollab) ì´ˆê¸°í™”
        super().__init__()

    # =========================================================
    # [Core] ğŸ§  ì±—ë´‡ ì§€ëŠ¥ & ê²€ìƒ‰ ì•Œê³ ë¦¬ì¦˜ (ì ˆëŒ€ ìˆ˜ì • ê¸ˆì§€ êµ¬ì—­)
    # =========================================================
    
    # [Helper] ë°ì´í„° ì •ê·œí™” (ê²€ìƒ‰ ë¡œì§ ë‚´ë¶€ì—ì„œ ì‚¬ìš©ë¨)
    def _normalize_tags(self, raw_tags):
        if not raw_tags or str(raw_tags).lower() in ['none', 'nan', 'null']:
            return "ê³µí†µ"
        tags = [t.strip() for t in str(raw_tags).split(',')]
        seen = set()
        clean_tags = []
        for tag in tags:
            if tag and tag not in seen:
                clean_tags.append(tag)
                seen.add(tag)
        return ", ".join(clean_tags) if clean_tags else "ê³µí†µ"

    def _clean_text(self, text):
        if not text or str(text).lower() in ['none', 'nan', 'null', 'ë¯¸ì§€ì •']:
            return "ë¯¸ì§€ì •"
        return str(text).strip()

    def keep_alive(self):
        try: self.supabase.table("knowledge_base").select("id").limit(1).execute()
        except: pass

    def get_penalty_counts(self):
        try:
            res = self.supabase.table("knowledge_blacklist").select("source_id").execute()
            return Counter([r['source_id'] for r in res.data])
        except: return {}

    def save_relevance_feedback(self, query, doc_id, t_name, score, query_vec=None, reason=None):
        try:
            payload = {
                "query_text": query.strip(),
                "doc_id": doc_id,
                "table_name": t_name,
                "relevance_score": score,
                "reason": reason
            }
            if query_vec:
                payload["query_embedding"] = query_vec
            self.supabase.table("relevance_feedback").insert(payload).execute()
            return True
        except: return False

    def get_semantic_context_blacklist(self, query_vec):
        try:
            res = self.supabase.rpc("match_relevance_feedback_batch", {
                "input_embedding": query_vec,
                "match_threshold": 0.95
            }).execute()
            if res.data:
                return {(item['table_name'], item['doc_id']) for item in res.data if item['relevance_score'] < 0}
            return set()
        except: return set()

    # [Admin] ë°ì´í„° ë¼ë²¨ë§ ìˆ˜ì • (ê´€ë¦¬ì ê¸°ëŠ¥)
    def update_record_labels(self, table_name, row_id, mfr, model, item):
        try:
            clean_mfr = self._clean_text(mfr)
            clean_model = self._clean_text(model)
            clean_item = self._normalize_tags(item)
            payload = {
                "manufacturer": clean_mfr, 
                "model_name": clean_model, 
                "measurement_item": clean_item, 
                "semantic_version": 1, 
                "review_required": False
            }
            res = self.supabase.table(table_name).update(payload).eq("id", row_id).execute()
            return (True, "ì„±ê³µ") if res.data else (False, "ì‹¤íŒ¨")
        except Exception as e: return (False, str(e))

    def update_file_labels(self, table_name, file_name, mfr, model, item):
        try:
            clean_mfr = self._clean_text(mfr)
            clean_model = self._clean_text(model)
            clean_item = self._normalize_tags(item)
            payload = {
                "manufacturer": clean_mfr, 
                "model_name": clean_model, 
                "measurement_item": clean_item, 
                "semantic_version": 1, 
                "review_required": False
            }
            res = self.supabase.table(table_name).update(payload).eq("file_name", file_name).or_(f'manufacturer.eq.ë¯¸ì§€ì •,manufacturer.is.null,manufacturer.eq.""').execute()
            return True, f"{len(res.data)}ê±´ ì¼ê´„ ë¶„ë¥˜ ì™„ë£Œ"
        except Exception as e: return False, str(e)

    def update_vector(self, table_name, row_id, vec):
        try: self.supabase.table(table_name).update({"embedding": vec}).eq("id", row_id).execute(); return True
        except: return False

    def delete_record(self, table_name, row_id):
        try:
            res = self.supabase.table(table_name).delete().eq("id", row_id).execute()
            return (True, "ì„±ê³µ") if res.data else (False, "ì‹¤íŒ¨")
        except Exception as e: return (False, str(e))

    # -------------------------------------------------------------------------
    # [Search Logic] ë²¡í„° ê²€ìƒ‰ + í‚¤ì›Œë“œ í•„í„°ë§ (ê¸°ì¡´ ë¡œì§ 100% ìœ ì§€)
    # -------------------------------------------------------------------------
    def match_filtered_db(self, rpc_name, query_vec, threshold, intent, query_text, context_blacklist=None):
        try:
            target_item = intent.get('target_item', 'ê³µí†µ')
            # 1. ë²¡í„° ê²€ìƒ‰ (RPC í˜¸ì¶œ)
            vector_results = self.supabase.rpc(rpc_name, {"query_embedding": query_vec, "match_threshold": threshold, "match_count": 40}).execute().data or []
            
            # 2. í‚¤ì›Œë“œ ë³´ì™„ ê²€ìƒ‰ (ëˆ„ë½ ë°©ì§€)
            keyword_results = []
            search_candidates = set()
            if target_item and target_item not in ['ê³µí†µ', 'ë¯¸ì§€ì •', 'none', 'unknown']:
                search_candidates.add(target_item.strip())
                search_candidates.add(target_item.replace(" ", ""))
            
            if not search_candidates:
                words = query_text.split()
                for w in words:
                    if len(w) >= 2 and w not in ['ì•Œë ¤ì¤˜', 'ì–´ë–»ê²Œ', 'êµì²´', 'ë°©ë²•', 'ì¤€ë¹„ë¬¼']:
                        search_candidates.add(w)
            
            if search_candidates:
                t_name = "manual_base" if "manual" in rpc_name else "knowledge_base"
                query_builder = self.supabase.table(t_name).select("*")
                or_conditions = []
                for kw in search_candidates:
                    if not kw: continue
                    if t_name == "manual_base":
                        or_conditions.append(f"measurement_item.ilike.%{kw}%")
                        or_conditions.append(f"model_name.ilike.%{kw}%")
                        or_conditions.append(f"content.ilike.%{kw}%")
                    else:
                        or_conditions.append(f"measurement_item.ilike.%{kw}%")
                        or_conditions.append(f"issue.ilike.%{kw}%")
                        or_conditions.append(f"solution.ilike.%{kw}%")
                
                if or_conditions:
                    final_filter = ",".join(or_conditions)
                    res = query_builder.or_(final_filter).limit(10).execute()
                    if res.data:
                        for d in res.data:
                            d['similarity'] = 0.99 # í‚¤ì›Œë“œ íˆíŠ¸ëŠ” ì ìˆ˜ ë§Œì 
                            keyword_results.append(d)

            # 3. ê²°ê³¼ ë³‘í•© ë° ì ìˆ˜ ë³´ì •
            merged_map = {}
            for d in vector_results: merged_map[d['id']] = d
            for d in keyword_results: merged_map[d['id']] = d 
                
            final_results_list = list(merged_map.values())
            filtered_results = []
            keywords = [k for k in query_text.split() if len(k) > 1]
            t_name = "manual_base" if "manual" in rpc_name else "knowledge_base"

            for d in final_results_list:
                if context_blacklist and (t_name, d['id']) in context_blacklist:
                    continue
                
                final_score = d.get('similarity') or 0
                content = (d.get('content') or d.get('solution') or "").lower()
                
                # í‚¤ì›Œë“œê°€ ë³¸ë¬¸ì— ìˆìœ¼ë©´ ê°€ì‚°ì 
                for kw in keywords:
                    if kw.lower() in content: final_score += 0.1
                
                d['similarity'] = final_score
                filtered_results.append(d)
                
            return filtered_results
        except Exception as e: return []

    # -------------------------------------------------------------------------
    # [Step 3] êµ¬ì›íˆ¬ìˆ˜: í‚¤ì›Œë“œ ê°•ì œ ë°œêµ´ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
    # -------------------------------------------------------------------------
    def search_keyword_fallback(self, query_text):
        keywords = [k for k in query_text.split() if len(k) >= 2]
        if not keywords: return []
        target_keyword = max(keywords, key=len)
        try:
            response = self.supabase.table("manual_base").select("*").or_(f"content.ilike.%{target_keyword}%,model_name.ilike.%{target_keyword}%").limit(5).execute()
            docs = response.data
            for d in docs:
                d['similarity'] = 0.98; d['source_table'] = 'manual_base'; d['is_verified'] = False 
            return docs
        except: return []

    # =========================================================
    # [Inheritance Note]
    # ì•„ë˜ ê¸°ëŠ¥ë“¤ì€ ëª¨ë‘ DBCollab(ë¶€ëª¨ í´ë˜ìŠ¤)ì—ì„œ ìƒì†ë°›ì•„ ì‚¬ìš©í•©ë‹ˆë‹¤.
    # 1. ì¬ê³ ê´€ë¦¬ (Inventory)
    # 2. ê²Œì‹œíŒ (Community)
    # 3. í˜‘ì—…/ì¼ì • (Schedule/Contact)
    # 4. ì§€ì‹ ê·¸ë˜í”„ (Graph CRUD)
    # =========================================================
