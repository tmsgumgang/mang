from collections import Counter
import re

class DBKnowledge:
    """
    [Core Brain] ì±—ë´‡ì˜ ì§€ëŠ¥, ì§€ì‹ ê²€ìƒ‰, RAG, ë©”íƒ€ë°ì´í„° ì¡°íšŒë¥¼ ë‹´ë‹¹í•©ë‹ˆë‹¤.
    V309: HAAS-4000 ê°™ì€ ëª¨ë¸ëª… ê²€ìƒ‰ ëˆ„ë½ ë°©ì§€ ë° Graph RAG ê¸°ëŠ¥ ì™„ë²½ ë³µêµ¬
    """
    def __init__(self, supabase_client):
        self.supabase = supabase_client

    # =========================================================
    # [Helper] ë°ì´í„° ì •ê·œí™”
    # =========================================================
    def _normalize_tags(self, raw_tags):
        if not raw_tags or str(raw_tags).lower() in ['none', 'nan', 'null']:
            return "ê³µí†µ"
        tags = [t.strip() for t in str(raw_tags).split(',')]
        seen = set()
        clean_tags = []
        for tag in tags:
            if tag and tag not in seen:
                clean_tags.append(tag)
                seen.add(tag)
        return ", ".join(clean_tags) if clean_tags else "ê³µí†µ"

    def _clean_text(self, text):
        if not text or str(text).lower() in ['none', 'nan', 'null', 'ë¯¸ì§€ì •']:
            return "ë¯¸ì§€ì •"
        return str(text).strip()

    def keep_alive(self):
        try: self.supabase.table("knowledge_base").select("id").limit(1).execute()
        except: pass

    # =========================================================
    # [Core] ê²€ìƒ‰ ì•Œê³ ë¦¬ì¦˜ (ì§€ëŠ¥ í•µì‹¬)
    # =========================================================
    def get_penalty_counts(self):
        try:
            res = self.supabase.table("knowledge_blacklist").select("source_id").execute()
            return Counter([r['source_id'] for r in res.data])
        except: return {}

    def get_semantic_context_blacklist(self, query_vec):
        try:
            res = self.supabase.rpc("match_relevance_feedback_batch", {
                "input_embedding": query_vec, "match_threshold": 0.95
            }).execute()
            if res.data:
                return {(item['table_name'], item['doc_id']) for item in res.data if item['relevance_score'] < 0}
            return set()
        except: return set()

    def match_filtered_db(self, rpc_name, query_vec, threshold, intent, query_text, context_blacklist=None):
        """
        [V309 í•µì‹¬ ìˆ˜ì •] ê²€ìƒ‰ì–´ ì¡°í•© ë¡œì§ ê°•í™”
        - ì˜ë„(Intent) ë¶„ì„ ê²°ê³¼ì™€ ë³„ê°œë¡œ, ì‚¬ìš©ìê°€ ì…ë ¥í•œ í•µì‹¬ í‚¤ì›Œë“œ(ëª¨ë¸ëª… ë“±)ë¥¼ 
          DBì—ì„œ ì§ì ‘ ê²€ìƒ‰(OR ì¡°ê±´)í•˜ì—¬ ëˆ„ë½ì„ ë°©ì§€í•©ë‹ˆë‹¤.
        """
        try:
            # 1. Vector Search (ì„ë² ë”© ê²€ìƒ‰)
            vector_results = self.supabase.rpc(rpc_name, {"query_embedding": query_vec, "match_threshold": threshold, "match_count": 40}).execute().data or []
            
            # 2. Keyword Search (í‚¤ì›Œë“œ ê°•ì œ ì†Œí™˜)
            search_candidates = set()
            
            # (1) ì˜ë„ëœ íƒ€ê²Ÿ ëª¨ë¸ëª…/í•­ëª© ì¶”ê°€
            if intent.get('target_model') and intent['target_model'] not in ['ë¯¸ì§€ì •', 'ê³µí†µ', 'none']:
                search_candidates.add(intent['target_model'].strip())
            if intent.get('target_item') and intent['target_item'] not in ['ë¯¸ì§€ì •', 'ê³µí†µ', 'none']:
                search_candidates.add(intent['target_item'].strip())
                
            # (2) [ë³µêµ¬ëœ ë¡œì§] ì‚¬ìš©ì ì§ˆë¬¸ì˜ í•µì‹¬ ë‹¨ì–´ ì¶”ì¶œ (ëª¨ë¸ëª… ëˆ„ë½ ë°©ì§€)
            clean_q = re.sub(r'[^\w\s-]', ' ', query_text)
            words = clean_q.split()
            ignore_words = ['ì•Œë ¤ì¤˜', 'ì–´ë–»ê²Œ', 'êµì²´', 'ë°©ë²•', 'ì¤€ë¹„ë¬¼', 'í•´ì¤˜', 'ìˆì–´', 'ë‚˜ìš”', 'ì¸ê°€ìš”', 'ëŠ˜ë¦¬ëŠ”', 'ë²•', 'ì¡°ì ˆ']
            
            for w in words:
                if len(w) >= 2 and w not in ignore_words:
                    search_candidates.add(w)
            
            keyword_results = []
            if search_candidates:
                t_name = "manual_base" if "manual" in rpc_name else "knowledge_base"
                
                # í•µì‹¬ í‚¤ì›Œë“œ(ìµœëŒ€ 5ê°œ)ì— ëŒ€í•´ DB ì§ì ‘ ê²€ìƒ‰
                for kw in list(search_candidates)[:5]:
                    if not kw: continue
                    try:
                        # ëª¨ë¸ëª…, í•­ëª©, ë‚´ìš© ì–´ë””ë“  í¬í•¨ë˜ë©´ ê°€ì ¸ì˜´
                        or_filter = f"model_name.ilike.%{kw}%,measurement_item.ilike.%{kw}%,content.ilike.%{kw}%"
                        if t_name == "knowledge_base":
                             or_filter = f"model_name.ilike.%{kw}%,measurement_item.ilike.%{kw}%,issue.ilike.%{kw}%,solution.ilike.%{kw}%"
                        
                        # ìƒìœ„ 10ê°œë§Œ ì¡°íšŒ (ì†ë„ ìµœì í™”)
                        res = self.supabase.table(t_name).select("*").or_(or_filter).limit(10).execute()
                        if res.data:
                            for d in res.data:
                                d['similarity'] = 0.99 # í‚¤ì›Œë“œë¡œ ì°¾ì€ ê±´ ì‹ ë¢°ë„ ìµœìƒ
                                keyword_results.append(d)
                    except: continue

            # 3. ê²°ê³¼ ë³‘í•© (Vector + Keyword)
            merged_map = {}
            for d in vector_results: merged_map[d['id']] = d
            for d in keyword_results: merged_map[d['id']] = d 
                
            final_results_list = list(merged_map.values())
            filtered_results = []
            
            # 4. ì ìˆ˜ ë³´ì • (Re-scoring)
            keywords_list = [k.lower() for k in query_text.split() if len(k) > 1]
            t_name = "manual_base" if "manual" in rpc_name else "knowledge_base"

            for d in final_results_list:
                if context_blacklist and (t_name, d['id']) in context_blacklist: continue
                
                final_score = d.get('similarity') or 0
                doc_full_text = (str(d.get('manufacturer','')) + str(d.get('model_name','')) + str(d.get('content','') or d.get('solution',''))).lower()
                
                for kw in keywords_list:
                    if kw in doc_full_text: final_score += 0.05
                    # [ì¤‘ìš”] ëª¨ë¸ëª…ì— í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ê°•ë ¥í•œ ê°€ì‚°ì  (HAAS-4000 êµ¬ì¶œ)
                    if kw in str(d.get('model_name','')).lower(): final_score += 0.2
                
                d['similarity'] = min(final_score, 1.0)
                filtered_results.append(d)
                
            return filtered_results
        except Exception as e:
            print(f"DB Search Error: {e}")
            return []

    # -------------------------------------------------------------------------
    # [Step 3] êµ¬ì›íˆ¬ìˆ˜: í‚¤ì›Œë“œ í´ë°± (utils_search.pyì—ì„œ í˜¸ì¶œë¨)
    # -------------------------------------------------------------------------
    def search_keyword_fallback(self, query_text):
        """[ë¹„ìƒìš©] ëª¨ë“  ê²€ìƒ‰ ì‹¤íŒ¨ ì‹œ ê°€ì¥ ê¸´ ë‹¨ì–´(ëª¨ë¸ëª… ì¶”ì •)ë¡œ ì „ìˆ˜ ì¡°ì‚¬"""
        keywords = [k for k in query_text.split() if len(k) >= 2]
        if not keywords: return []
        
        target_keyword = max(keywords, key=len)
        try:
            # ë§¤ë‰´ì–¼ë¿ë§Œ ì•„ë‹ˆë¼ ì§€ì‹ë…¸íŠ¸ê¹Œì§€ í™•ì¥ ê²€ìƒ‰í•˜ë©´ ë” ì¢‹ì§€ë§Œ, 
            # ì¼ë‹¨ ì‚¬ìš©ìë‹˜ì˜ ì›ë³¸ ì½”ë“œëŒ€ë¡œ ë§¤ë‰´ì–¼ ìš°ì„  ê²€ìƒ‰ì„ ìœ ì§€í•©ë‹ˆë‹¤.
            res = self.supabase.table("manual_base").select("*").or_(f"content.ilike.%{target_keyword}%,model_name.ilike.%{target_keyword}%").limit(5).execute()
            docs = res.data
            for d in docs: 
                d['similarity'] = 0.98  # ë©´ì±… íŠ¹ê¶Œ ì ìˆ˜ ë¶€ì—¬
                d['source_table'] = 'manual_base'
                d['is_verified'] = False 
            return docs
        except: return []

    # =========================================================
    # [Knowledge Graph] ì§€ì‹ ê·¸ë˜í”„ (utils_search.py ë“±ì—ì„œ í˜¸ì¶œë¨)
    # =========================================================
    def save_knowledge_triples(self, doc_id, triples):
        if not triples: return False
        try:
            data = []
            for t in triples:
                if t.get('source') and t.get('target'):
                    data.append({"source": self._clean_text(t['source']), "relation": t.get('relation', 'related_to'), "target": self._clean_text(t['target']), "doc_id": doc_id})
            if data: self.supabase.table("knowledge_graph").insert(data).execute(); return True
            return False
        except: return False

    def search_graph_relations(self, keyword):
        try: return self.supabase.table("knowledge_graph").select("*").or_(f"source.ilike.%{keyword}%,target.ilike.%{keyword}%").limit(20).execute().data
        except: return []

    def update_graph_triple(self, rel_id, new_source, new_relation, new_target):
        try:
            payload = {"source": self._clean_text(new_source), "relation": new_relation, "target": self._clean_text(new_target)}
            res = self.supabase.table("knowledge_graph").update(payload).eq("id", rel_id).execute()
            return True if res.data else False
        except: return False

    def delete_graph_triple(self, rel_id):
        try: self.supabase.table("knowledge_graph").delete().eq("id", rel_id).execute(); return True
        except: return False

    def bulk_rename_graph_node(self, old_name, new_name, target_scope="all"):
        try:
            count = 0
            if target_scope in ["source", "all"]:
                res = self.supabase.table("knowledge_graph").update({"source": self._clean_text(new_name)}).eq("source", old_name).execute()
                if res.data: count += len(res.data)
            if target_scope in ["target", "all"]:
                res = self.supabase.table("knowledge_graph").update({"target": self._clean_text(new_name)}).eq("target", old_name).execute()
                if res.data: count += len(res.data)
            return True, count
        except: return False, 0

    # =========================================================
    # [Metadata Utils] ì±—ë´‡ì´ ì°¸ì¡°í•˜ëŠ” ë©”íƒ€ë°ì´í„° (ì§€ëŠ¥ì˜ ì›ì²œ)
    # =========================================================
    def get_doc_metadata_by_id(self, doc_id, source_type):
        try:
            t_name = "knowledge_base" if source_type == "knowledge" else "manual_base"
            res = self.supabase.table(t_name).select("manufacturer, model_name, measurement_item").eq("id", doc_id).execute()
            if res.data: return res.data[0]
            return {}
        except: return {}

    def search_inventory_for_chat(self, query_text):
        try:
            stop_words = ['ì¬ê³ ', 'ìˆ˜ëŸ‰', 'ëª‡ê°œ', 'ëª‡', 'ê°œ', 'ìˆì–´', 'ìˆë‚˜ìš”', 'ì•Œë ¤ì¤˜', 'í™•ì¸', 'ì¡°íšŒ', 'ì–´ë””', 'ìˆë‹ˆ', 'í˜„í™©', 'ë³´ì—¬ì¤˜', 'ì†Œëª¨í’ˆ']
            keywords = [k for k in query_text.split() if k not in stop_words and len(k) >= 2]
            if not keywords: return None
            res = self.supabase.table("inventory_items").select("*").or_(",".join([f"item_name.ilike.%{kw}%" for kw in keywords])).execute()
            if not res.data: return f"ğŸ” **'{', '.join(keywords)}'**ì— ëŒ€í•œ ì¬ê³  ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤."
            msg = f"ğŸ“¦ **ì¬ê³  ê²€ìƒ‰ ê²°ê³¼ ({len(res.data)}ê±´):**\n"
            for item in res.data[:10]: msg += f"- [{item.get('category')}] **{item.get('item_name')}**: {item.get('current_qty')}ê°œ ({item.get('location')})\n"
            return msg
        except Exception as e: return f"ì¬ê³  ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}"

    # =========================================================
    # [Knowledge Manage] ì§€ì‹ ê´€ë¦¬ (Admin ê¸°ëŠ¥)
    # =========================================================
    def search_knowledge_for_admin(self, keyword):
        try: return self.supabase.table("knowledge_base").select("*").or_(f"issue.ilike.%{keyword}%,solution.ilike.%{keyword}%").order("created_at", desc=True).limit(20).execute().data
        except: return []

    def update_knowledge_item(self, doc_id, new_issue, new_sol, mfr, model, item):
        try:
            from logic_ai import get_embedding
            payload = {"issue": new_issue, "solution": new_sol, "manufacturer": self._clean_text(mfr), "model_name": self._clean_text(model), "measurement_item": self._normalize_tags(item), "embedding": get_embedding(f"ì¦ìƒ: {new_issue}\ní•´ê²°: {new_sol}"), "semantic_version": 2}
            res = self.supabase.table("knowledge_base").update(payload).eq("id", doc_id).execute()
            return True if res.data else False
        except: return False

    def update_record_labels(self, table_name, row_id, mfr, model, item):
        try:
            payload = {"manufacturer": self._clean_text(mfr), "model_name": self._clean_text(model), "measurement_item": self._normalize_tags(item), "semantic_version": 1, "review_required": False}
            res = self.supabase.table(table_name).update(payload).eq("id", row_id).execute()
            return (True, "ì„±ê³µ") if res.data else (False, "ì‹¤íŒ¨")
        except Exception as e: return (False, str(e))
    
    def update_file_labels(self, table_name, file_name, mfr, model, item):
        try:
            payload = {"manufacturer": self._clean_text(mfr), "model_name": self._clean_text(model), "measurement_item": self._normalize_tags(item), "semantic_version": 1, "review_required": False}
            res = self.supabase.table(table_name).update(payload).eq("file_name", file_name).or_(f'manufacturer.eq.ë¯¸ì§€ì •,manufacturer.is.null,manufacturer.eq.""').execute()
            return True, f"{len(res.data)}ê±´ ì¼ê´„ ë¶„ë¥˜ ì™„ë£Œ"
        except Exception as e: return False, str(e)
    
    def promote_to_knowledge(self, issue, solution, mfr, model, item, author="ìµëª…"):
        try:
            from logic_ai import get_embedding
            payload = {"domain": "ê¸°ìˆ ì§€ì‹", "issue": issue, "solution": solution, "embedding": get_embedding(issue), "semantic_version": 1, "is_verified": True, "manufacturer": self._clean_text(mfr), "model_name": self._clean_text(model), "measurement_item": self._normalize_tags(item), "registered_by": author}
            res = self.supabase.table("knowledge_base").insert(payload).execute()
            return (True, "ì„±ê³µ") if res.data else (False, "ì‹¤íŒ¨")
        except Exception as e: return (False, str(e))
    
    def update_vector(self, table_name, row_id, vec):
        try: self.supabase.table(table_name).update({"embedding": vec}).eq("id", row_id).execute(); return True
        except: return False

    def delete_record(self, table_name, row_id):
        try:
            res = self.supabase.table(table_name).delete().eq("id", row_id).execute()
            return (True, "ì„±ê³µ") if res.data else (False, "ì‹¤íŒ¨")
        except Exception as e: return (False, str(e))
    
    def save_relevance_feedback(self, query, doc_id, t_name, score, query_vec=None, reason=None):
        try:
            payload = {
                "query_text": query.strip(), "doc_id": doc_id, "table_name": t_name,
                "relevance_score": score, "reason": reason
            }
            if query_vec: payload["query_embedding"] = query_vec
            self.supabase.table("relevance_feedback").insert(payload).execute()
            return True
        except: return False
