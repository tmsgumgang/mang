import re
import json
import requests
import streamlit as st
import google.generativeai as genai
from prompts import PROMPTS 

# [V248] ì„ë² ë”© ë¡œì§ ì•ˆì •í™”: REST API ì§ì ‘ í˜¸ì¶œ (404 ì—ëŸ¬ ì›ì²œ ì°¨ë‹¨)
@st.cache_data(show_spinner=False)
def get_embedding(text):
    """
    - êµ¬í˜• ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ ë‚´ë¶€ ë²„ê·¸(404 ì—ëŸ¬)ë¥¼ í”¼í•˜ê¸° ìœ„í•´,
    - ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ê±°ì¹˜ì§€ ì•Šê³  êµ¬ê¸€ ì„œë²„ë¡œ ì§ì ‘ ë°ì´í„°ë¥¼ ì´ì„œ ë²¡í„°ë¥¼ ë°›ì•„ì˜µë‹ˆë‹¤.
    """
    cleaned_text = clean_text_for_db(text)
    if not cleaned_text: 
        return []

    try:
        api_key = st.secrets["GEMINI_API_KEY"]
        # ìµœì‹  ëª¨ë¸ë¶€í„° êµ¬í˜• ëª¨ë¸ê¹Œì§€ ìˆœì„œëŒ€ë¡œ êµ¬ê¸€ ì„œë²„ì— ì§ì ‘ ì°”ëŸ¬ë´…ë‹ˆë‹¤.
        models_to_try = ["text-embedding-004", "embedding-001"]
        last_error = ""

        for model_name in models_to_try:
            try:
                # êµ¬ê¸€ ì„œë²„ ë‹¤ì´ë ‰íŠ¸ ì£¼ì†Œ
                url = f"https://generativelanguage.googleapis.com/v1beta/models/{model_name}:embedContent?key={api_key}"
                headers = {'Content-Type': 'application/json'}
                payload = {
                    "model": f"models/{model_name}",
                    "content": {
                        "parts": [{"text": cleaned_text}]
                    }
                }
                
                # ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ì´ ì§ì ‘ í†µì‹ !
                response = requests.post(url, headers=headers, json=payload)
                
                if response.status_code == 200:
                    res_data = response.json()
                    if 'embedding' in res_data and 'values' in res_data['embedding']:
                        return res_data['embedding']['values']
                else:
                    last_error = f"{response.status_code} - {response.text}"
                    print(f"âš ï¸ {model_name} ì§ì ‘ í˜¸ì¶œ ì‹¤íŒ¨: {last_error}")
                    
            except Exception as req_err:
                last_error = str(req_err)
                print(f"âš ï¸ {model_name} ë„¤íŠ¸ì›Œí¬ ì—ëŸ¬: {last_error}")

        # ëª¨ë“  ëª¨ë¸ ì‹œë„ ì‹¤íŒ¨ ì‹œ
        error_msg = f"ğŸš¨ AI ì„ë² ë”© ì„œë²„ ì§ì ‘ í†µì‹  ì‹¤íŒ¨.\nì›ì¸: {last_error}"
        print(error_msg)
        st.error(error_msg)
        return []

    except Exception as e:
        print(f"âŒ ì„ë² ë”© í•¨ìˆ˜ ë‚´ë¶€ ì—ëŸ¬: {e}")
        st.error(f"ì„ë² ë”© ë¡œì§ ì˜¤ë¥˜: {e}")
        return []

def semantic_split_v143(text, target_size=1200, min_size=600):
    flat_text = " ".join(text.split())
    sentences = re.split(r'(?<=[.!?])\s+', flat_text)
    chunks, current_chunk = [], ""
    for sentence in sentences:
        if len(current_chunk) + len(sentence) <= target_size:
            current_chunk += " " + sentence
        else:
            if current_chunk: chunks.append(current_chunk.strip())
            current_chunk = sentence
    if current_chunk:
        if len(current_chunk) < min_size and chunks:
            chunks[-1] = chunks[-1] + " " + current_chunk.strip()
        else: chunks.append(current_chunk.strip())
    return chunks

def clean_text_for_db(text):
    if not text: return ""
    text = text.replace("\u0000", "")
    return "".join(ch for ch in text if ch.isprintable() or ch in ['\n', '\r', '\t']).strip()

def extract_json(text):
    try:
        cleaned = re.sub(r'```json\s*|```', '', text).strip()
        return json.loads(cleaned)
    except: return None

# --------------------------------------------------------------------------------
# [V206] ìë™ í‚¤ì›Œë“œ íƒœê¹… (App.pyì˜ ì–´ëŒ‘í„°ì™€ í˜¸í™˜)
# --------------------------------------------------------------------------------
def extract_metadata_ai(ai_model, content):
    try:
        prompt = PROMPTS["extract_metadata"].format(content=content[:2000])
        res = ai_model.generate_content(prompt)
        return extract_json(res.text)
    except: return None

@st.cache_data(ttl=3600, show_spinner=False)
def analyze_search_intent(_ai_model, query):
    default_intent = {
        "target_mfr": "ë¯¸ì§€ì •", 
        "target_model": "ë¯¸ì§€ì •", 
        "target_item": "ê³µí†µ",
        "target_action": "ì¼ë°˜"
    }
    try:
        prompt = PROMPTS["search_intent"].format(query=query)
        res = _ai_model.generate_content(prompt)
        intent_res = extract_json(res.text)
        if intent_res and isinstance(intent_res, dict):
            return intent_res
        return default_intent
    except:
        return default_intent

@st.cache_data(ttl=3600, show_spinner=False)
def quick_rerank_ai(_ai_model, query, results, intent):
    if not results: return []
    safe_intent = intent if (intent and isinstance(intent, dict)) else {"target_mfr": "ë¯¸ì§€ì •", "target_item": "ê³µí†µ"}
    
    candidates = []
    for r in results[:5]:
        candidates.append({
            "id": r.get('id'), 
            "mfr": r.get('manufacturer'), 
            "item": r.get('measurement_item'),
            "content": (r.get('content') or r.get('solution'))[:200]
        })

    prompt = PROMPTS["rerank_score"].format(
        query=query, 
        mfr=safe_intent.get('target_mfr'), 
        item=safe_intent.get('target_item'), 
        candidates=json.dumps(candidates, ensure_ascii=False)
    )
    
    try:
        res = _ai_model.generate_content(prompt)
        scores = extract_json(res.text)
        score_map = {item['id']: item['score'] for item in scores}
        for r in results: r['rerank_score'] = score_map.get(r['id'], 0)
        return sorted(results, key=lambda x: x['rerank_score'], reverse=True)
    except: return results

def generate_3line_summary_stream(ai_model, query, results):
    if not results:
        yield "ê²€ìƒ‰ ê²°ê³¼ê°€ ë¶€ì¡±í•˜ì—¬ ìš”ì•½ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        return

    top_doc = results[0]
    top_content = f"â˜…ìµœìš°ì„ ì°¸ê³ ìë£Œ(Fact Source): {top_doc.get('content') or top_doc.get('solution')}"
    
    other_context = []
    for r in results[1:3]:
        other_context.append(f"- ë³´ì¡°ìë£Œ: {r.get('content') or r.get('solution')}")
    
    full_context = [top_content] + other_context
    
    prompt = PROMPTS["summary_fact_lock"].format(
        query=query, 
        context=json.dumps(full_context, ensure_ascii=False)
    )
    
    # [V245] ì–´ëŒ‘í„°ê°€ stream ìš”ì²­ë„ ì²˜ë¦¬í•˜ë„ë¡ ì„¤ê³„ë¨
    response = ai_model.generate_content(prompt, stream=True)
    
    # ì‹ í˜• ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ ìŠ¤íŠ¸ë¦¼ ì‘ë‹µ ì²˜ë¦¬
    for chunk in response:
        if chunk.text:
            yield chunk.text

@st.cache_data(ttl=3600, show_spinner=False)
def unified_rerank_and_summary_ai(_ai_model, query, results, intent):
    if not results: return [], "ê´€ë ¨ ì§€ì‹ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
    safe_intent = intent if (intent and isinstance(intent, dict)) else {"target_mfr": "ë¯¸ì§€ì •", "target_item": "ê³µí†µ"}
    candidates = [{"id":r['id'],"content":(r.get('content')or r.get('solution'))[:300]} for r in results[:5]]
    
    prompt = PROMPTS["unified_rerank"].format(
        query=query, 
        safe_intent=safe_intent, 
        candidates=candidates
    )
    
    try:
        res = _ai_model.generate_content(prompt)
        parsed = extract_json(res.text)
        score_map = {item['id']: item['score'] for item in parsed.get('scores', [])}
        for r in results: r['rerank_score'] = score_map.get(r['id'], 0)
        return sorted(results, key=lambda x: x['rerank_score'], reverse=True), parsed.get('summary', "ìš”ì•½ ë¶ˆê°€")
    except: return results, "ì˜¤ë¥˜ ë°œìƒ"

def generate_relevant_summary(ai_model, query, data):
    prompt = PROMPTS["deep_report"].format(
        query=query, 
        data=data
    )
    res = ai_model.generate_content(prompt)
    return res.text

# --------------------------------------------------------------------------------
# [NEW V246] Graph RAG ê´€ê³„ ì¶”ì¶œ ì—”ì§„ (ì œì¡°ì‚¬ ê´€ê³„ ì¶”ê°€)
# --------------------------------------------------------------------------------
def extract_triples_from_text(ai_model, text):
    """
    í…ìŠ¤íŠ¸ì—ì„œ (ì£¼ì–´) -> [ê´€ê³„] -> (ëª©ì ì–´) íŠ¸ë¦¬í”Œì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
    """
    graph_prompt = f"""
    You are an expert Data Engineer specializing in Knowledge Graphs.
    Analyze the provided technical text and extract relationships between entities.
    
    Target Entities: Device, Part, Symptom, Cause, Solution, Action, Value, Location, Manufacturer.
    Target Relations: 
    - causes (ì›ì¸ì´ë‹¤)
    - part_of (ì˜ ë¶€í’ˆì´ë‹¤)
    - located_in (ì— ìœ„ì¹˜í•œë‹¤)
    - solved_by (ë¡œ í•´ê²°ëœë‹¤)
    - has_status (ìƒíƒœë¥¼ ê°€ì§„ë‹¤)
    - requires (ì„ í•„ìš”ë¡œ í•œë‹¤)
    - manufactured_by (ì´ ì œì¡°í–ˆë‹¤)

    IMPORTANT: 
    - Entities MUST be single nouns or short phrases.
    - Return ONLY a JSON array of objects.
    
    Text to Analyze:
    {text[:2500]}
    """
    
    try:
        res = ai_model.generate_content(graph_prompt)
        triples = extract_json(res.text)
        if triples and isinstance(triples, list):
            return triples
        return []
    except Exception as e:
        print(f"Graph Extraction Error: {e}")
        return []
