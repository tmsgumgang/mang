import re
import json
import google.generativeai as genai
import streamlit as st
from prompts import PROMPTS 

@st.cache_data(show_spinner=False)
def get_embedding(text):
    """[V243] ì„ë² ë”© ëª¨ë¸ Fallback ë¡œì§ ì¶”ê°€"""
    cleaned_text = clean_text_for_db(text)
    try:
        result = genai.embed_content(model="models/text-embedding-004", content=cleaned_text, task_type="retrieval_document")
        return result['embedding']
    except Exception:
        try:
            result = genai.embed_content(model="models/embedding-001", content=cleaned_text, task_type="retrieval_document")
            return result['embedding']
        except Exception: return []

def semantic_split_v143(text, target_size=1200, min_size=600):
    flat_text = " ".join(text.split())
    sentences = re.split(r'(?<=[.!?])\s+', flat_text)
    chunks, current_chunk = [], ""
    for sentence in sentences:
        if len(current_chunk) + len(sentence) <= target_size:
            current_chunk += " " + sentence
        else:
            if current_chunk: chunks.append(current_chunk.strip())
            current_chunk = sentence
    if current_chunk:
        if len(current_chunk) < min_size and chunks:
            chunks[-1] = chunks[-1] + " " + current_chunk.strip()
        else: chunks.append(current_chunk.strip())
    return chunks

def clean_text_for_db(text):
    if not text: return ""
    text = text.replace("\u0000", "")
    return "".join(ch for ch in text if ch.isprintable() or ch in ['\n', '\r', '\t']).strip()

def extract_json(text):
    try:
        cleaned = re.sub(r'```json\s*|```', '', text).strip()
        if cleaned.startswith('{') and not cleaned.endswith('}'): cleaned += '}'
        return json.loads(cleaned)
    except: return None

# --------------------------------------------------------------------------------
# [V206] ìë™ í‚¤ì›Œë“œ íƒœê¹…(Auto-Tagging) ì—”ì§„
# --------------------------------------------------------------------------------
def extract_metadata_ai(ai_model, content):
    try:
        prompt = PROMPTS["extract_metadata"].format(content=content[:2000])
        res = ai_model.generate_content(prompt)
        return extract_json(res.text)
    except: return None

@st.cache_data(ttl=3600, show_spinner=False)
def analyze_search_intent(_ai_model, query):
    default_intent = {"target_mfr": "ë¯¸ì§€ì •", "target_model": "ë¯¸ì§€ì •", "target_item": "ê³µí†µ", "target_action": "ì¼ë°˜"}
    try:
        prompt = PROMPTS["search_intent"].format(query=query)
        res = _ai_model.generate_content(prompt)
        intent_res = extract_json(res.text)
        if intent_res and isinstance(intent_res, dict): return intent_res
        return default_intent
    except: return default_intent

@st.cache_data(ttl=3600, show_spinner=False)
def quick_rerank_ai(_ai_model, query, results, intent):
    """
    [V311] ë¦¬ë­í‚¹ ì‹œ ê·¸ë˜í”„ ë°ì´í„° ê°€ì¤‘ì¹˜ ë¶€ì—¬ ë° ë©”íƒ€ë°ì´í„° ì£¼ì…
    """
    if not results: return []
    safe_intent = intent if (intent and isinstance(intent, dict)) else {"target_mfr": "ë¯¸ì§€ì •", "target_item": "ê³µí†µ"}
    
    candidates = []
    # ìƒìœ„ 8ê°œ ì •ë°€ ë¶„ì„
    for r in results[:8]:
        mfr = r.get('manufacturer', '')
        model = r.get('model_name', '')
        item = r.get('measurement_item', '')
        raw_content = (r.get('content') or r.get('solution') or "")[:400]
        
        # [í•µì‹¬] ê·¸ë˜í”„ ë°ì´í„° ì‹ë³„ ë° ê°•ì¡°
        is_graph = (r.get('source_table') == 'knowledge_graph') or ('ì§€ì‹ê·¸ë˜í”„' in str(mfr))
        if is_graph:
            context_str = f"ğŸ”¥[í•µì‹¬ì¸ê³¼ê´€ê³„/ì§€ì‹ê·¸ë˜í”„] {raw_content}"
        else:
            context_str = f"[{mfr} {model} ({item})] {raw_content}"
        
        candidates.append({"id": r.get('id'), "info": context_str})

    prompt = PROMPTS["rerank_score"].format(
        query=query, 
        mfr=safe_intent.get('target_mfr'), 
        item=safe_intent.get('target_item'), 
        candidates=json.dumps(candidates, ensure_ascii=False)
    )
    
    try:
        res = _ai_model.generate_content(prompt)
        scores = extract_json(res.text)
        if scores and isinstance(scores, list):
            score_map = {str(item['id']): item.get('score', 0) for item in scores}
            for r in results: r['rerank_score'] = score_map.get(str(r['id']), 0)
            return sorted(results, key=lambda x: x.get('rerank_score', 0), reverse=True)
        return results
    except: return results

def generate_3line_summary_stream(ai_model, query, results):
    """
    [V311] ë‹µë³€ ìƒì„± ì‹œ ê·¸ë˜í”„ ë°ì´í„°(Key Insight)ì™€ ì¼ë°˜ ë¬¸ì„œë¥¼ êµ¬ë¶„í•˜ì—¬ ì œê³µ
    """
    if not results:
        yield "ê²€ìƒ‰ ê²°ê³¼ê°€ ë¶€ì¡±í•˜ì—¬ ìš”ì•½ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        return

    full_context = []
    
    # 1. ë°ì´í„°ë¥¼ ê·¸ë˜í”„(ì¡±ë³´)ì™€ ì¼ë°˜ ë¬¸ì„œë¡œ ë¶„ë¥˜
    graph_data = []
    manual_data = []
    
    for r in results:
        is_graph = (r.get('source_table') == 'knowledge_graph') or ('ì§€ì‹ê·¸ë˜í”„' in str(r.get('manufacturer', '')))
        mfr = r.get('manufacturer', 'ë¯¸ì§€ì •')
        model = r.get('model_name', 'ê³µí†µ')
        content = (r.get('content') or r.get('solution') or "").strip()
        
        if is_graph:
            # [Smart Point] ê·¸ë˜í”„ ë°ì´í„°ëŠ” "ê²°ì •ì  ë‹¨ì„œ"ë¡œ í¬ì¥
            graph_data.append(f"ğŸ’¡ ê²°ì •ì  ë‹¨ì„œ (Key Insight/Graph): {content}")
        else:
            # ì¼ë°˜ ë¬¸ì„œëŠ” ì¶œì²˜ ëª…ì‹œ
            manual_data.append(f"- ë¬¸ì„œìë£Œ: [{mfr} {model}] {content}")
            
    # 2. ë¬¸ë§¥ ì¡°í•©: ê·¸ë˜í”„ ë°ì´í„°ë¥¼ ìµœìƒë‹¨ì— ë°°ì¹˜í•˜ì—¬ AIê°€ ë¨¼ì € ì½ê²Œ í•¨ (ì•µì»¤ë§ íš¨ê³¼)
    # ê·¸ë˜í”„ ë°ì´í„°ëŠ” ìµœëŒ€ 3ê°œ, ë§¤ë‰´ì–¼ì€ ìµœëŒ€ 4ê°œë¡œ ì œí•œí•˜ì—¬ Context Window íš¨ìœ¨í™”
    final_context_list = graph_data[:3] + manual_data[:4]
    
    prompt = PROMPTS["summary_fact_lock"].format(
        query=query, 
        context=json.dumps(final_context_list, ensure_ascii=False)
    )
    
    response = ai_model.generate_content(prompt, stream=True)
    for chunk in response:
        if chunk.text: yield chunk.text

@st.cache_data(ttl=3600, show_spinner=False)
def unified_rerank_and_summary_ai(_ai_model, query, results, intent):
    if not results: return [], "ê´€ë ¨ ì§€ì‹ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
    safe_intent = intent if (intent and isinstance(intent, dict)) else {"target_mfr": "ë¯¸ì§€ì •", "target_item": "ê³µí†µ"}
    
    candidates = []
    for r in results[:6]:
        is_graph = (r.get('source_table') == 'knowledge_graph')
        meta = "ğŸ”¥[ì§€ì‹ê·¸ë˜í”„]" if is_graph else f"[{r.get('manufacturer','')} {r.get('model_name','')}]"
        content = (r.get('content') or r.get('solution'))[:300]
        candidates.append({"id": r['id'], "content": f"{meta} {content}"})
    
    prompt = PROMPTS["unified_rerank"].format(query=query, safe_intent=safe_intent, candidates=candidates)
    
    try:
        res = _ai_model.generate_content(prompt)
        parsed = extract_json(res.text)
        score_map = {item['id']: item['score'] for item in parsed.get('scores', [])}
        for r in results: r['rerank_score'] = score_map.get(r['id'], 0)
        return sorted(results, key=lambda x: x['rerank_score'], reverse=True), parsed.get('summary', "ìš”ì•½ ë¶ˆê°€")
    except: return results, "ì˜¤ë¥˜ ë°œìƒ"

def generate_relevant_summary(ai_model, query, data):
    prompt = PROMPTS["deep_report"].format(query=query, data=data)
    res = ai_model.generate_content(prompt)
    return res.text

# [V252] Graph RAG ê´€ê³„ ì¶”ì¶œ ì—”ì§„
def extract_triples_from_text(ai_model, text):
    graph_prompt = f"""
    You are an expert Data Engineer specializing in Knowledge Graphs.
    Target Relations: causes, part_of, consumable_of, is_facility_of, is_a, included_in, located_in, solved_by, has_status, requires, manufactured_by.
    Return ONLY a JSON array. Format: [{{"source": "A", "relation": "causes", "target": "B"}}]
    Text: {text[:3500]}
    """
    try:
        res = ai_model.generate_content(graph_prompt)
        triples = extract_json(res.text)
        return triples if isinstance(triples, list) else []
    except: return []
